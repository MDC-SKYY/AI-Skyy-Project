{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Ollama -q\n",
    "%pip install llama_index.llms.ollama -q\n",
    "%pip install llama_index.core -q\n",
    "%pip install llama_index.embeddings.ollama -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-DO: Get a list of the urls corresponding to each link text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/wd/84g4f7_55r39pnh89vfg3s480000gn/T/ipykernel_11323/4130321290.py:13: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all items in the directory and create a list of pathlib.PosixPath entries each detailing a path to a .txt training file\n",
    "#.txt files are expected to be numered\n",
    "\n",
    "def get_training_files(path):\n",
    "\n",
    "   directory = Path(path)\n",
    "   training_files = []\n",
    "\n",
    "   for file in directory.iterdir():\n",
    "      training_files.append(file)\n",
    "\n",
    "   # Sort the list of pathlib.PosixPath entries based on numeric part of the .txt file\n",
    "   \"\"\"\"\"\n",
    "   1) files.sort(: This starts the sorting operation on the files list. The sort() method sorts the elements of the list in place.\n",
    "   \n",
    "   2) key=lambda x:: This is a key function used to customize the sorting order. Here, lambda x: defines an anonymous function that takes one argument x.\n",
    "   \n",
    "   3) re.search(r'\\d+', x.stem): This part uses the re.search() function from the re module to search for a pattern in the file name.\n",
    "      \\d+ is a regular expression pattern that matches one or more digits. x.stem gives the base name of the file without the extension.\n",
    "\n",
    "   4) .group(): This method returns the matched part of the string. In this case, it returns the matched digits as a string.\n",
    "   \n",
    "   5) int(...): This converts the matched string of digits to an integer. This is necessary because we want to sort the files numerically,\n",
    "      not lexicographically.\n",
    "\n",
    "   Putting it all together, this line sorts the list of files (files) based on the numeric part extracted from each file's name.\n",
    "   The lambda function extracts this numeric part using a regular expression and converts it to an integer, which is then used as the sorting key.\n",
    "\"\"\"\"\"\n",
    "   training_files.sort(key=lambda x: int(re.search(r'\\d+', x.stem).group()))\n",
    "\n",
    "   return training_files\n",
    "\n",
    "training_set = get_training_files(\"/Users/raulriveromartinez/Desktop/Skyy AI/training_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a list of str that contains only the names of the .txt files without their full directory\n",
    "\n",
    "def get_filenames(training_files):   \n",
    "\n",
    "   filenames = []\n",
    "   \n",
    "   for file in training_files:\n",
    "      filenames.append(file.name)\n",
    "\n",
    "   return filenames\n",
    "\n",
    "filenames_set = get_filenames(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_contents(training_files):\n",
    "     \n",
    "     # Create an empty list to store the contents of each text file in the training dataset\n",
    "     files_contents = []\n",
    "\n",
    "     # Iterate through each text file in the training dataset and add its contents to the files_contents list\n",
    "     for file in training_files:\n",
    "          with open(file, \"r\") as contents:\n",
    "               txt_contents = contents.read()\n",
    "               files_contents.append(txt_contents)\n",
    "\n",
    "     return files_contents\n",
    "\n",
    "contents_set = get_files_contents(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the filename list and the file_content lists on a dictionary with two key-value pairs: filename and contents\n",
    "\n",
    "def get_contents_dict(filenames, files_contents):\n",
    "\n",
    "    AllData = []\n",
    "    x = 0\n",
    "    for i in range(0,len(filenames)):\n",
    "        data = {'filename':filenames[i],'content':files_contents[i]}\n",
    "        AllData.append(data)\n",
    "\n",
    "    return AllData\n",
    "\n",
    "files_content_dicts = get_contents_dict(filenames_set, contents_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allData is the name of the dictionary storing the filename and contents of each file as key value pairs\n",
    "\n",
    "def chunk_data_as_sentences(allData):\n",
    "\t# list to store all data as individual sentences with their corresponding filename\n",
    "\tsystem_data = list()\n",
    "\t# process each file's content in the dictionary\n",
    "\tfor i in allData:\n",
    "\t\t# segment (chunk) each file's contents into individual sentences\n",
    "\t\tsentences = nltk.sent_tokenize(i['content'])\n",
    "\t\t# store each sentence with its filename\n",
    "\t\tfor s in sentences:\n",
    "\t\t\tsystem_data.append({'filename': i['filename'], 'content': s})\n",
    "\t\t\t\n",
    "\treturn system_data\n",
    "\n",
    "chunked_files_content_dicts = chunk_data_as_sentences(files_content_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding function to convert sentence into vector\n",
    "\n",
    "def set_local_model(model_name = \"llama3\"):\n",
    "    #TO-DO:add script to check if model is installed\n",
    "    local_model = OllamaEmbedding(model_name = model_name, base_url = 'http://localhost:11434')\n",
    "    return local_model\n",
    "\n",
    "local_model = set_local_model(\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_record = list()\n",
    "\n",
    "for i in chunked_files_content_dicts[:5]:\n",
    "\t\t# function call need to match completed implementation\n",
    "\t\tembedding = local_model.get_query_embedding(i['content'])\n",
    "\t\trecord = {'filename': i['filename'], 'content': i['content'], 'vector': embedding}\n",
    "\t\tsystem_record.append(record)\n",
    "\t\t\n",
    "for i in system_record: print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings for the \"chunked\" data\n",
    "# pass allData as dictionary\n",
    "\n",
    "def embed_sentences(allData):\n",
    "\t# create a list to store each sentence in each file with its\n",
    "\t# filename, the sentence text, and its embedding\n",
    "\tsystem_record = list()\n",
    "\tfor i in allData:\n",
    "\t\t# function call need to match completed implementation\n",
    "\t\tembedding = local_model.get_query_embedding(i['content'])\n",
    "\t\trecord = {'filename': i['filename'], 'content': i['content'], 'vector': embedding}\n",
    "\t\tsystem_record.append(record)\n",
    "\n",
    "\twith open('mdc_index.json', 'w') as outfile:\n",
    "        \tjson.dump(record, outfile, indent=2)\n",
    "\n",
    "embed_sentences(chunked_files_content_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
